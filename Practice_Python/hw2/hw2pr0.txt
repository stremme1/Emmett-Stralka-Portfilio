Is it possible for something with unexamined presumptions / limited self-knowledge to be conscious?

There is no agreed-upon definition of "consciousness," so it is difficult to answer this question definitively. However, it seems reasonable to say that something with unexamined presumptions or limited self-knowledge could still be conscious in some sense, even if its level of consciousness is not as rich or nuanced as that of something with a greater degree of self-awareness.

Will the next five decades see software deserving of the adjective "conscious"?

This is difficult to predict, as it largely depends on how one defines "consciousness." If one takes a relatively broad definition of consciousness, then it is possible that software could meet this criterion in the next five decades. However, if one takes a narrower definition of consciousness, then it is less likely that any software will meet this criterion in the next five decades.

Is "conscious software" something to be sought, to be avoided, or neither of the above?

If consciousness is defined as self-awareness, then there may not be any solid reason to seek or avoid conscious software. In that case, there may be reasons to seek or avoid conscious software, depending on one's views about the ethical implications of artificial intelligence. The moment humans are not used for creative thought is the moment humans are inefficient.
